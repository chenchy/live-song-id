{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we demonstrate that the effect of training a CNN with one layer, initialized by PCA coefficients, on pairs of CQT images, whether or not they are similar or non-similar. In the experiment, we used the PCA coefficients trained on original HashPrint approach's dataset. The annotation dataset is used for training to distinguish similar pairs and non-similar pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name to be something meaningful. \n",
    "# It will be part of the name of the model to be saved.\n",
    "EXPERIMENT_NAME = \"[hashnet-kapre]\"\n",
    "DEBUG = True        # Set to True if you want to see outputs used for debugging \n",
    "verbose = True      # Set to True if you want to see the loss function per epoch in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Input, Concatenate, Layer, BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import datetime\n",
    "import keras\n",
    "import pickle\n",
    "import logging\n",
    "import h5py\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import scipy.io\n",
    "\n",
    "import songdb\n",
    "songdb.DB_DIR = os.path.join(os.curdir, \"paired_annotation_data\")\n",
    "data_dict = songdb.data_dict\n",
    "\n",
    "from preprocess import *\n",
    "from model import build_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "# File Handler\n",
    "fhandler = logging.FileHandler(filename='[{:}.log'.format(EXPERIMENT_NAME), mode='w')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "\n",
    "# Output stream Handler\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "if DEBUG: \n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "else:     \n",
    "    logger.setLeveL(logging.INFO)\n",
    "    ch.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Keras\n",
    "\n",
    "In this code, we will use tensorflow backend on Keras because it's relatively faster than Theano. We'll use the image data format in the form of `(n_channel, width, height)`. Please look at the Keras documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11 23:48:59,136 - root - DEBUG - Keras backend is tensorflow.\n",
      "2018-04-11 23:48:59,139 - root - DEBUG - Keras image data format is channels_first\n"
     ]
    }
   ],
   "source": [
    "assert K.backend() == 'tensorflow'\n",
    "logger.debug(\"Keras backend is tensorflow.\")\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "assert K.image_data_format() == 'channels_first'\n",
    "logger.debug(\"Keras image data format is channels_first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "We load the pair-annotation dataset. Here, we generate an array of of shapes `(n_pairs, 2, 20, 121)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11 23:48:59,183 - root - DEBUG - Load Paired-annotation data\n",
      "2018-04-11 23:48:59,196 - root - DEBUG - \t data.type = <class 'generator'>\n",
      "2018-04-11 23:48:59,198 - root - DEBUG - \t data = <generator object get_ref_query_pairs at 0x00000200AF7A8C50>\n",
      "2018-04-11 23:50:06,015 - root - DEBUG - Song 1: Finish current pairs with shape = (272, 2, 20, 121)\n",
      "2018-04-11 23:50:06,019 - root - DEBUG - Number of samples for the current song: 272\n",
      "2018-04-11 23:51:03,705 - root - DEBUG - Song 2: Finish current pairs with shape = (264, 2, 20, 121)\n",
      "2018-04-11 23:51:03,714 - root - DEBUG - Number of samples for the current song: 264\n",
      "2018-04-11 23:51:30,090 - root - DEBUG - Song 3: Finish current pairs with shape = (100, 2, 20, 121)\n",
      "2018-04-11 23:51:30,096 - root - DEBUG - Number of samples for the current song: 100\n",
      "2018-04-11 23:51:52,367 - root - DEBUG - Song 4: Finish current pairs with shape = (100, 2, 20, 121)\n",
      "2018-04-11 23:51:52,371 - root - DEBUG - Number of samples for the current song: 100\n",
      "2018-04-11 23:52:49,068 - root - DEBUG - Song 5: Finish current pairs with shape = (221, 2, 20, 121)\n",
      "2018-04-11 23:52:49,083 - root - DEBUG - Number of samples for the current song: 221\n",
      "2018-04-11 23:54:16,510 - root - DEBUG - Song 6: Finish current pairs with shape = (253, 2, 20, 121)\n",
      "2018-04-11 23:54:16,516 - root - DEBUG - Number of samples for the current song: 253\n",
      "2018-04-11 23:55:07,227 - root - DEBUG - Song 7: Finish current pairs with shape = (224, 2, 20, 121)\n",
      "2018-04-11 23:55:07,257 - root - DEBUG - Number of samples for the current song: 224\n",
      "2018-04-11 23:56:01,763 - root - DEBUG - Song 8: Finish current pairs with shape = (200, 2, 20, 121)\n",
      "2018-04-11 23:56:01,766 - root - DEBUG - Number of samples for the current song: 200\n",
      "2018-04-11 23:56:17,582 - root - DEBUG - Song 9: Finish current pairs with shape = (65, 2, 20, 121)\n",
      "2018-04-11 23:56:17,586 - root - DEBUG - Number of samples for the current song: 65\n",
      "2018-04-11 23:57:10,632 - root - DEBUG - Song 10: Finish current pairs with shape = (211, 2, 20, 121)\n",
      "2018-04-11 23:57:10,636 - root - DEBUG - Number of samples for the current song: 211\n",
      "2018-04-11 23:57:34,778 - root - DEBUG - Song 11: Finish current pairs with shape = (100, 2, 20, 121)\n",
      "2018-04-11 23:57:34,786 - root - DEBUG - Number of samples for the current song: 100\n",
      "2018-04-11 23:57:57,979 - root - DEBUG - Song 12: Finish current pairs with shape = (100, 2, 20, 121)\n",
      "2018-04-11 23:57:57,982 - root - DEBUG - Number of samples for the current song: 100\n",
      "2018-04-12 00:00:09,095 - root - DEBUG - Song 13: Finish current pairs with shape = (559, 2, 20, 121)\n",
      "2018-04-12 00:00:09,099 - root - DEBUG - Number of samples for the current song: 559\n",
      "2018-04-12 00:02:07,336 - root - DEBUG - Song 14: Finish current pairs with shape = (568, 2, 20, 121)\n",
      "2018-04-12 00:02:07,341 - root - DEBUG - Number of samples for the current song: 568\n",
      "2018-04-12 00:03:54,435 - root - DEBUG - Song 15: Finish current pairs with shape = (336, 2, 20, 121)\n",
      "2018-04-12 00:03:54,439 - root - DEBUG - Number of samples for the current song: 336\n",
      "2018-04-12 00:04:22,652 - root - DEBUG - Song 16: Finish current pairs with shape = (112, 2, 20, 121)\n",
      "2018-04-12 00:04:22,655 - root - DEBUG - Number of samples for the current song: 112\n",
      "2018-04-12 00:06:45,336 - root - DEBUG - Song 17: Finish current pairs with shape = (595, 2, 20, 121)\n",
      "2018-04-12 00:06:45,339 - root - DEBUG - Number of samples for the current song: 595\n",
      "2018-04-12 00:08:24,619 - root - DEBUG - Song 18: Finish current pairs with shape = (353, 2, 20, 121)\n",
      "2018-04-12 00:08:24,622 - root - DEBUG - Number of samples for the current song: 353\n",
      "2018-04-12 00:09:12,141 - root - DEBUG - Song 19: Finish current pairs with shape = (195, 2, 20, 121)\n",
      "2018-04-12 00:09:12,148 - root - DEBUG - Number of samples for the current song: 195\n",
      "2018-04-12 00:10:17,011 - root - DEBUG - Song 20: Finish current pairs with shape = (268, 2, 20, 121)\n",
      "2018-04-12 00:10:17,014 - root - DEBUG - Number of samples for the current song: 268\n",
      "2018-04-12 00:10:17,023 - root - DEBUG - There are 5096 number of pairs\n"
     ]
    }
   ],
   "source": [
    "logger.debug(\"Load Paired-annotation data\")\n",
    "# data = annotations.get_ref_query_pairs(\"taylorswift\", beat_width=5900)\n",
    "data = data_dict()[\"taylorswift\"]\n",
    "logger.debug(\"\\t data.type = {:}\".format(type(data)))\n",
    "logger.debug(\"\\t data = {:}\".format(data))\n",
    "\n",
    "songPairs = []\n",
    "totalPairs = 0 \n",
    "for ref, query in data:\n",
    "    # Preprocess ref, query\n",
    "    ref_model = normalization_model(audio_len=ref.shape[1])\n",
    "    query_model = normalization_model(audio_len=query.shape[1])\n",
    "    \n",
    "    ref = run_preprocessing(ref_model, ref)\n",
    "    query = run_preprocessing(query_model, query)\n",
    "    \n",
    "    # Get pairs for each ref + query\n",
    "    curr_pairs = []\n",
    "    for i in range(ref.shape[0]):\n",
    "        ref_array = np.asfarray(ref[i], dtype='float32')\n",
    "        refQ = preprocess(librosa.cqt(ref_array, sr = 22050, fmin=130.81, n_bins=121, bins_per_octave=24, hop_length=96), 3).T\n",
    "        \n",
    "        query_array = np.asfarray(query[i], dtype='float32')\n",
    "        queryQ = preprocess(librosa.cqt(query_array, sr = 22050, fmin=130.81, n_bins=121, bins_per_octave=24, hop_length=96), 3).T\n",
    "        \n",
    "        Q_pair = np.append(np.array([refQ]), np.array([queryQ]), axis=0)[np.newaxis, :]\n",
    "        curr_pairs.append(Q_pair[:,:,28:48,:]) \n",
    "    curr_pairs = np.vstack(tuple(curr_pairs))\n",
    "    totalPairs += curr_pairs.shape[0]\n",
    "    songPairs.append(curr_pairs)\n",
    "    logger.debug(\"Song {:}: Finish current pairs with shape = {:}\".format(len(songPairs), curr_pairs.shape))\n",
    "    logger.debug(\"Number of samples for the current song: {:}\".format(len(curr_pairs)))\n",
    "\n",
    "logger.debug(\"There are {:} number of pairs\".format(totalPairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-12 00:10:17,045 - root - DEBUG - Finish loading paired-annotation data\n",
      "2018-04-12 00:10:17,049 - root - DEBUG - => There are 14 songs for training [63.52 percent]\n",
      "2018-04-12 00:10:17,050 - root - DEBUG - => There are 6 songs for validation\n"
     ]
    }
   ],
   "source": [
    "p_train = .6\n",
    "cutOffIdx = 0\n",
    "currentNumberOfPairs = 0\n",
    "for song in songPairs:\n",
    "    currentNumberOfPairs += song.shape[0]\n",
    "    cutOffIdx += 1\n",
    "    if currentNumberOfPairs >= totalPairs * p_train:\n",
    "        break\n",
    "\n",
    "data_train = tuple(songPairs[:cutOffIdx])\n",
    "data_val = tuple(songPairs[cutOffIdx:])\n",
    "logger.debug(\"Finish loading paired-annotation data\")\n",
    "logger.debug(\"=> There are {:} songs for training [{:.2f} percent]\".format(len(data_train), (currentNumberOfPairs/totalPairs * 100.0)))\n",
    "logger.debug(\"=> There are {:} songs for validation\".format(len(data_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Obtain a matrix $X_1$, $X_2$, $[w_{ij}]$, and $[s_{ij}]$ where $X_1$ and $X_2$ are pairs of images of shape \n",
    "`(n_samples, 1, width, height)`; $[w_{ij}]$ is the weight vector of shape `(n_samples, 1)` which describes the weight of each pair; and $[s_{ij}]$ is a binary vector of shape `(n_samples,1)` indicating whether inputs $i$ and $j$ are the same.\n",
    "\n",
    "__Add similar pairs__\n",
    "\n",
    "We add similar pairs based on the dataset we have.\n",
    "\n",
    "__Add non-similar pairs__\n",
    "\n",
    "We add non-similar pairs by randomly selecting two CQT frames from the dataset. Note that we say two CQT pairs are not similar when they come from different songs only.\n",
    "\n",
    "__Data Post-processing__\n",
    "\n",
    "Convert $X_1$, $X_2$, $[s_{ij}]$ in to numpy arrays.\n",
    "\n",
    "__Calculate weights__\n",
    "\n",
    "Calculate the weight vector $[w_{ij}]$. The term $w_{ij}$ is defined as \n",
    "\n",
    "$$\n",
    "    w_{ij} = \\begin{cases}\n",
    "    \\frac{|S|}{|S_{1}|} &\\text{if $x^{(i)}$ is similar to $x^{(j)}$}\\\\\n",
    "    \\frac{|S|}{|S_{0}|} &\\text{if $x^{(i)}$ is not similar to $x^{(j)}$}\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(data, nNonSimilar=1000):\n",
    "#     nPairs = data.shape[0] \n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    S = []\n",
    "    groundTruth = []\n",
    "    \n",
    "    # add similar pairs\n",
    "    for songId in range(len(data)):\n",
    "        nPairs = data[songId].shape[0]\n",
    "        for idx in range(nPairs):\n",
    "            X1.append(np.expand_dims(data[songId][idx][0][:][:], axis=0))\n",
    "            X2.append(np.expand_dims(data[songId][idx][1][:][:], axis=0))\n",
    "            groundTruth.append(((songId,idx),(songId,idx)))\n",
    "            S.append(True)\n",
    "        \n",
    "    # add non-similar pairs\n",
    "    for i in range(nNonSimilar):\n",
    "        [songId1, songId2] = np.random.choice(len(data), 2, replace=False)\n",
    "        nPairs1 = data[songId1].shape[0]\n",
    "        nPairs2 = data[songId2].shape[0]\n",
    "\n",
    "        [idx1] = np.random.choice(nPairs1, 1)\n",
    "        [idx2] = np.random.choice(nPairs2, 1)\n",
    "        X1.append(np.expand_dims(data[songId1][idx1][0][:][:], axis=0))\n",
    "        X2.append(np.expand_dims(data[songId2][idx2][1][:][:], axis=0))\n",
    "        S.append((songId1,idx1) == (songId2,idx2))\n",
    "        groundTruth.append(((songId1,idx1),(songId2,idx2)))\n",
    "        \n",
    "    # post-processing\n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "    S = np.array(S).reshape(-1,1)\n",
    "    # calculate weights\n",
    "    n_sim = len(S[S==True])\n",
    "    w_sim   = len(X1) / n_sim             # w_ij for similar pairs\n",
    "    w_insim = len(X1) / (len(X1) - n_sim)  # w_ij for dissimilar pairs\n",
    "    W = S*w_sim + (~S)*w_insim          # w vector for pairs\n",
    "    S = S.astype('float32')\n",
    "    \n",
    "    logger.debug(\"X1 shape: {:}\".format(X1.shape))\n",
    "    logger.debug(\"X2 shape: {:}\".format(X2.shape))\n",
    "    logger.debug(\"S shape: {:}\".format(S.shape))\n",
    "    logger.debug(\"W shape: {:}\".format(W.shape))\n",
    "    logger.debug(\"There are {:} out of {:} pairs that are similar [{:.2f} percent]\".format(n_sim, \n",
    "                                                                                           len(X1),\n",
    "                                                                                           100.0*n_sim/len(X1)))\n",
    "    \n",
    "    return X1, X2, S, W, groundTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-12 00:10:18,171 - root - DEBUG - X1 shape: (7237, 1, 20, 121)\n",
      "2018-04-12 00:10:18,184 - root - DEBUG - X2 shape: (7237, 1, 20, 121)\n",
      "2018-04-12 00:10:18,187 - root - DEBUG - S shape: (7237, 1)\n",
      "2018-04-12 00:10:18,190 - root - DEBUG - W shape: (7237, 1)\n",
      "2018-04-12 00:10:18,196 - root - DEBUG - There are 3237 out of 7237 pairs that are similar [44.73 percent]\n",
      "2018-04-12 00:10:18,668 - root - DEBUG - X1 shape: (3859, 1, 20, 121)\n",
      "2018-04-12 00:10:18,674 - root - DEBUG - X2 shape: (3859, 1, 20, 121)\n",
      "2018-04-12 00:10:18,687 - root - DEBUG - S shape: (3859, 1)\n",
      "2018-04-12 00:10:18,693 - root - DEBUG - W shape: (3859, 1)\n",
      "2018-04-12 00:10:18,696 - root - DEBUG - There are 1859 out of 3859 pairs that are similar [48.17 percent]\n"
     ]
    }
   ],
   "source": [
    "X1, X2, S, W, groundTruth = generate_training_data(data_train, nNonSimilar=4000)\n",
    "X1_val, X2_val, S_val, W_val, groundTruth_val = generate_training_data(data_val, nNonSimilar=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPairs = X1.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "__Define inputs__\n",
    "\n",
    "Create a HashNet model with four inputs: two images tensors, similarity vector, and weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 20\n",
    "height = 121\n",
    "n_channel = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_a    = Input(shape=(n_channel, width, height), name='image_1')\n",
    "image_b    = Input(shape=(n_channel, width, height), name='image_2')\n",
    "similarity = Input(shape=(1,), name=\"similarity\")\n",
    "weight     = Input(shape=(1,), name=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pre-trained model__\n",
    "\n",
    "Use a pre-trained model here to obtain intermediate values from the pre-trained model. We call this pre-trained model by `base_model`. The `base_model` is generally used to convert a CQT image to some representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-12 00:10:20,143 - root - DEBUG - Load model from model.mat\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model.mat'\n",
    "model = scipy.io.loadmat(model_path)\n",
    "logger.debug(\"Load model from {:}\".format(model_path))\n",
    "evecs = np.array(model['eigvecs']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 20\n",
    "pca_matrix = np.array([vec.reshape((m, -1)) for vec in evecs])\n",
    "delta = 4\n",
    "base_model = build_model(pca_matrix, (width, height), delta=delta, compute_delta=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove this block of code if you want to have original hashprint model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_layer = base_model.layers[0]\n",
    "PCA_layer.trainable = False\n",
    "base_model.pop()\n",
    "base_model.add(BatchNormalization(input_shape=(n_channel, width, height)))\n",
    "base_model.add(Convolution2D(64, (20,1)))\n",
    "base_model.add(Dropout(0.5))\n",
    "base_model.add(Convolution2D(64, (1,3), padding='same'))\n",
    "base_model.add(Dropout(0.5))\n",
    "base_model.add(Convolution2D(64, (1,3), padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1860\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[0;32m   1862\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[0;32m    708\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    710\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m    996\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m    998\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1866\u001b[0m                     prog=prog)\n\u001b[1;32m-> 1867\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot.exe\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4647cf87fd49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'base_model.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Save base_model architecture to base_model.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mSVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \"\"\"\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# pydot raises a generic Exception here,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# so no specific class can be caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[0;32m     32\u001b[0m                           ' and graphviz for `pydotprint` to work.')\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "plot_model(base_model, show_shapes=True, to_file='base_model.png')\n",
    "logger.debug(\"Save base_model architecture to base_model.png\")\n",
    "SVG(model_to_dot(base_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obtain intermediate values__\n",
    "\n",
    "Intermediate values are values directly after the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain intermediate tensor\n",
    "intermediate_a = base_model(image_a)\n",
    "intermediate_b = base_model(image_b)\n",
    "\n",
    "# Flatten the layer\n",
    "flatten_a = Flatten()(intermediate_a)\n",
    "flatten_b = Flatten()(intermediate_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Append a layer for binary encoding__\n",
    "\n",
    "This layer has a `tanh` activation defined in the same way as HashNet paper. Note that initial beta value, number of bits, and `alpha` in the block below are hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta         = 1            # Initial beta value\n",
    "numberOfBits = 64\n",
    "alpha        = 0.01         # Set to be something small because it prevents the loss function from blowing up.\n",
    "logger.debug(\"[HashNet Model] beta = {:}, numberOfBits = {:}, alpha = {:}\".format(beta, numberOfBits, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_activation(x):\n",
    "    \"\"\"\n",
    "    Our own defined activation function\n",
    "    \"\"\"\n",
    "    global beta\n",
    "    return K.tanh(beta * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizedLossLayer(Layer):\n",
    "    \"\"\"\n",
    "    Our own defined layer for keeping track of loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomizedLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def my_loss(self, encoded_a, encoded_b, similarity, weight):\n",
    "        global alpha\n",
    "        x = encoded_a\n",
    "        y = encoded_b\n",
    "        dot_product = K.sum(x * y, axis=-1, keepdims=True)\n",
    "        logger.debug(dot_product)\n",
    "        return K.sum(weight * (K.log(1 + K.exp(alpha * dot_product)) - alpha * similarity * dot_product))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        encoded_a = inputs[0]\n",
    "        encoded_b = inputs[1]\n",
    "        similarity = inputs[2]\n",
    "        weight = inputs[3]\n",
    "        loss = self.my_loss(encoded_a, encoded_b, similarity, weight)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return K.ones_like(similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_layer = Dense(numberOfBits, activation='tanh')\n",
    "encoded_a = hash_layer(flatten_a)\n",
    "encoded_b = hash_layer(flatten_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create a model for training__\n",
    "\n",
    "We construct a model using 2 representations derived by the same base model and similarity and weight. The architecture of the model is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss layer with 4 inputs\n",
    "loss = CustomizedLossLayer()([encoded_a, encoded_b, similarity, weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model that has 4 inputs and outputs loss\n",
    "model = Model(inputs=[image_a, image_b, similarity, weight], outputs=[loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with our own defined loss function.\n",
    "# Note that loss function is already defined in a layer, so\n",
    "# `zero_loss` here doesn't actually do anything.\n",
    "\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "logger.debug(\"Use Adam optimizer with lr = 1e-2\")\n",
    "adam = Adam(lr=1e-2, decay=1e-2)\n",
    "model.compile(optimizer=adam, loss=zero_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualize model architecture__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, to_file='model.png')\n",
    "logger.debug(\"Save model architecture to model.png\")\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance before training\n",
    "\n",
    "For the purpose of understanding the performance of this algorithm, we find the baseline result by computing the hamming distance between two representations before training and then plotting the corresponding histogram. Based on the above diagram, we use the representation at layers named `flatten_1` and `flatten_2`, before the fully connected layer defined by HashNet paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model = Model(image_a, encoded_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(X1, X2, groundTruth, model):\n",
    "    plt.figure()\n",
    "    hist_sim = []\n",
    "    hist_non_sim = []\n",
    "\n",
    "    pred_X1 = (model.predict(X1) > 0)\n",
    "    pred_X2 = (model.predict(X2) > 0)\n",
    "    for idx in range(len(X1)):\n",
    "        score = np.count_nonzero(pred_X1[idx] != pred_X2[idx])\n",
    "        similarity = (groundTruth[idx][0] == groundTruth[idx][1])\n",
    "\n",
    "        if similarity:\n",
    "            hist_sim.append(score)\n",
    "        else:\n",
    "            hist_non_sim.append(score)\n",
    "\n",
    "    bins = np.linspace(0, max(max(hist_sim), max(hist_non_sim)), 100)\n",
    "    plt.ylim((0, 400))\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"$|x-y|$, e.g. Hamming distance\")\n",
    "    plt.hist(hist_sim, bins, alpha=0.5, label='Similar pairs', color='blue')\n",
    "    plt.hist(hist_non_sim, bins, alpha=0.5, label='Non-similar pairs', color='orange')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize(X1, X2, groundTruth, representation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(X1_val, X2_val, groundTruth_val, representation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_X1 = (representation_model.predict(X1) > 0)\n",
    "pred_X2 = (representation_model.predict(X2) > 0)\n",
    "outputs_sim    = []\n",
    "outputs_nonsim = []\n",
    "for idx in range(len(pred_X1)):\n",
    "    score = np.count_nonzero(pred_X1[idx] != pred_X2[idx])\n",
    "    similarity = (groundTruth[idx][0] == groundTruth[idx][1])\n",
    "    if similarity:\n",
    "        outputs_sim.append((score, X1[idx], X2[idx]))\n",
    "    else:\n",
    "        outputs_nonsim.append((score, X1[idx], X2[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outputs_sim = sorted(outputs_sim, key=lambda x: x[0]);\n",
    "outputs_nonsim = sorted(outputs_nonsim, key=lambda x: x[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar pairs\n",
    "# closest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_sim[i][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_sim[i][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "\n",
    "plt.suptitle(\"Similar pairs from training data that are most similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# furthest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_sim[-i-1][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_sim[-i-1][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    \n",
    "plt.suptitle(\"Similar pairs from training data that are least similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Similar pairs\n",
    "# closest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_nonsim[i][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_nonsim[i][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "\n",
    "plt.suptitle(\"Non-similar pairs from training data that are most similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# furthest    \n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_nonsim[-i-1][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_nonsim[-i-1][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    \n",
    "plt.suptitle(\"Non-similar pairs from training data that are least similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_X1 = (representation_model.predict(X1_val) > 0)\n",
    "pred_X2 = (representation_model.predict(X2_val) > 0)\n",
    "outputs_sim    = []\n",
    "outputs_nonsim = []\n",
    "for idx in range(len(pred_X1)):\n",
    "    score = np.count_nonzero(pred_X1[idx] != pred_X2[idx])\n",
    "    similarity = (groundTruth_val[idx][0] == groundTruth_val[idx][1])\n",
    "    if similarity:\n",
    "        outputs_sim.append((score, X1_val[idx], X2_val[idx]))\n",
    "    else:\n",
    "        outputs_nonsim.append((score, X1_val[idx], X2_val[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_sim = sorted(outputs_sim, key=lambda x: x[0]);\n",
    "outputs_nonsim = sorted(outputs_nonsim, key=lambda x: x[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar pairs\n",
    "# closest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_sim[i][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_sim[i][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "\n",
    "plt.suptitle(\"Similar pairs from validation data that are most similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# furthest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_sim[-i-1][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_sim[-i-1][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    \n",
    "plt.suptitle(\"Similar pairs from validation data that are least similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Similar pairs\n",
    "# closest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_nonsim[i][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_nonsim[i][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "\n",
    "plt.suptitle(\"Non-similar pairs from validation data that are most similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# furthest    \n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_nonsim[-i-1][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_nonsim[-i-1][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "\n",
    "plt.suptitle(\"Non-similar pairs from validation data that are least similar\", fontsize=16)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the input shape\n",
    "logger.debug(\"Ready to train the model\")\n",
    "logger.debug(\"X1 shape: {:}\".format(X1.shape))\n",
    "logger.debug(\"X2 shape: {:}\".format(X2.shape))\n",
    "logger.debug(\"S shape: {:}\".format(S.shape))\n",
    "logger.debug(\"W shape: {:}\".format(W.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_iterations = 100\n",
    "logger.debug(\"[Training] number_of_iterations = {:}\".format(number_of_iterations))\n",
    "beta = 1\n",
    "beta_factor = 1.5\n",
    "logger.debug(\"[Training] beta_factor = {:}\".format(beta_factor))\n",
    "number_of_epochs = 100\n",
    "logger.debug(\"[Training] number_of_epochs = {:}\".format(number_of_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "m = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_iterations):\n",
    "    logger.debug(\"Iteration {:} (current beta = {:})\".format(m, beta))\n",
    "    m += 1\n",
    "\n",
    "    # Train a model with 4 inputs and 1 dummy output.\n",
    "    model.fit({'image_1': X1, 'image_2': X2, 'similarity': S, 'weight': W}, [S], \n",
    "              validation_data=({'image_1': X1_val, 'image_2': X2_val, 'similarity': S_val, 'weight': W_val}, [S_val]),\n",
    "              verbose=verbose, \n",
    "              epochs=number_of_epochs)\n",
    "    beta *= beta_factor\n",
    "\n",
    "    representation_model = Model(image_a, encoded_a)\n",
    "    visualize(X1, X2, groundTruth, representation_model)\n",
    "    visualize(X1_val, X2_val, groundTruth_val, representation_model)\n",
    "\n",
    "    train_loss = model.evaluate({'image_1': X1, 'image_2': X2, 'similarity': S, 'weight': W}, [S])\n",
    "    test_loss  = model.evaluate({'image_1': X1_val, 'image_2': X2_val, 'similarity': S_val, 'weight': W_val}, [S_val])\n",
    "    logger.debug(\"[Iteration = {:}] Training Loss = {:}, Testing Loss = {:}\".format(m, train_loss, test_loss))\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"learning curve\")\n",
    "plt.plot(range(1,len(train_losses)+1), train_losses, '.', color='orange', label='Training')\n",
    "plt.plot(range(1,len(test_losses)+1), test_losses, '.', color='blue', label='Validation')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"No. of Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = str(datetime.date.today()) + EXPERIMENT_NAME + \"[Iteration={:}]\".format(m)\n",
    "logger.debug(\"Save model to {:}.h5\".format(MODEL_NAME))\n",
    "model.save(MODEL_NAME + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obtain a representation model__\n",
    "\n",
    "This representation model is the same as the trained model above without the loss layer at the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model = Model(image_a, encoded_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample output\n",
    "test_model = Model(image_a, flatten_a)\n",
    "pred_X1 = test_model.predict(X1)\n",
    "pred_X2 = test_model.predict(X2)\n",
    "plt.imshow(pred_X1[:200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the representation before threshold.\n",
    "test_model = Model(image_a, encoded_a)\n",
    "pred_X1 = test_model.predict(X1)\n",
    "pred_X2 = test_model.predict(X2)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(pred_X1[400:700])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bit importance of X1\n",
    "plt.plot(np.sum(pred_X1 > 0, axis=0) / len(pred_X1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bit importance of X2\n",
    "plt.plot(np.sum(pred_X2 > 0, axis=0) / len(pred_X2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance\n",
    "Calculate the hamming distance input pairs and plot the corresponding histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the layer before Dense\n",
    "representation_model = Model(image_a, flatten_a)\n",
    "visualize(X1_val, X2_val, groundTruth_val, representation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the dense layer\n",
    "representation_model = Model(image_a, encoded_a)\n",
    "visualize(X1_val, X2_val, groundTruth_val, representation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_X1 = (representation_model.predict(X1) > 0)\n",
    "pred_X2 = (representation_model.predict(X2) > 0)\n",
    "outputs_sim    = []\n",
    "outputs_nonsim = []\n",
    "for idx in range(len(pred_X1)):\n",
    "    score = np.count_nonzero(pred_X1[idx] != pred_X2[idx])\n",
    "    similarity = (groundTruth[idx][0] == groundTruth[idx][1])\n",
    "    if similarity:\n",
    "        outputs_sim.append((score, X1[idx], X2[idx]))\n",
    "    else:\n",
    "        outputs_nonsim.append((score, X1[idx], X2[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outputs_sim = sorted(outputs_sim, key=lambda x: x[0]);\n",
    "outputs_nonsim = sorted(outputs_nonsim, key=lambda x: x[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar pairs\n",
    "# closest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_sim[i][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_sim[i][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "\n",
    "plt.suptitle(\"Similar pairs from training data that are most similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# furthest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_sim[-i-1][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_sim[-i-1][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    \n",
    "plt.suptitle(\"Similar pairs from training data that are least similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Similar pairs\n",
    "# closest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_nonsim[i][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_nonsim[i][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "\n",
    "plt.suptitle(\"Non-similar pairs from training data that are most similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# furthest    \n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_nonsim[-i-1][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_nonsim[-i-1][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    \n",
    "plt.suptitle(\"Non-similar pairs from training data that are least similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_X1 = (representation_model.predict(X1_val) > 0)\n",
    "pred_X2 = (representation_model.predict(X2_val) > 0)\n",
    "outputs_sim    = []\n",
    "outputs_nonsim = []\n",
    "for idx in range(len(pred_X1)):\n",
    "    score = np.count_nonzero(pred_X1[idx] != pred_X2[idx])\n",
    "    similarity = (groundTruth_val[idx][0] == groundTruth_val[idx][1])\n",
    "    if similarity:\n",
    "        outputs_sim.append((score, X1_val[idx], X2_val[idx]))\n",
    "    else:\n",
    "        outputs_nonsim.append((score, X1_val[idx], X2_val[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_sim = sorted(outputs_sim, key=lambda x: x[0]);\n",
    "outputs_nonsim = sorted(outputs_nonsim, key=lambda x: x[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Similar pairs\n",
    "# closest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_sim[i][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_sim[i][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "\n",
    "plt.suptitle(\"Similar pairs from validation data that are most similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_sim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# furthest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_sim[-i-1][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_sim[-i-1][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    \n",
    "plt.suptitle(\"Similar pairs from validation data that are least similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Similar pairs\n",
    "# closest\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_nonsim[i][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_nonsim[i][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "\n",
    "plt.suptitle(\"Non-similar pairs from validation data that are most similar\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# furthest    \n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    plt.imshow(outputs_nonsim[-i-1][1][0,:,:].reshape(20,-1), cmap='jet')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(outputs_nonsim[-i-1][2][0,:,:].reshape(20,-1), cmap='jet')\n",
    "\n",
    "plt.suptitle(\"Non-similar pairs from validation data that are least similar\", fontsize=16)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(representation_model.predict(X1_val).reshape(64,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Architecture Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate for MRR\n",
    "\n",
    "This evaluation is not similar to the original hashprint approach, as it truncates the query to be the same size as the size of `base_model.input_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from preprocess import get_allpaths\n",
    "from search import calculateMRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUTPUT_DIR = \"/data/mjenrungrot/live-song-output/\" # Location to store *.npy files\n",
    "AUDIO_DIR = \"/home/mirlab/Data/SoftLinks/\"              # Location of audio files input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_files = get_allpaths('taylorswift', os.path.join(AUDIO_DIR, 'Lists/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(DATA_OUTPUT_DIR, 'taylorswift_cqtList.txt'), 'w')\n",
    "for ref_file in ref_files:\n",
    "    logger.debug(\"Process {:}\".format(ref_file))\n",
    "    audio_file = os.path.join(AUDIO_DIR, ref_file + '.wav')\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    Q = librosa.cqt(y, sr=sr, fmin=130.81, n_bins=121, bins_per_octave=24, hop_length=96)\n",
    "    \n",
    "    # Preprocess CQT to logQ\n",
    "    ds = 3\n",
    "    absQ = np.absolute(Q)\n",
    "    smoothQ = np.zeros((absQ.shape[0], absQ.shape[1]//ds))\n",
    "    for row in range(absQ.shape[0]):\n",
    "        smoothQ[row] = np.convolve(absQ[row], [1/ds]*ds, 'valid')[0:absQ.shape[1]-ds+1:ds]\n",
    "    logQ = np.log(1+1000000*smoothQ)\n",
    "    \n",
    "    output_file = os.path.join(DATA_OUTPUT_DIR, ref_file + '.npy')\n",
    "    np.save(output_file, logQ)\n",
    "    f.write(ref_file + '.npy\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = os.path.join(DATA_OUTPUT_DIR, \"taylorswift_db.hdf5\")\n",
    "db = h5py.File(db_path, mode='w')\n",
    "f = open(os.path.join(DATA_OUTPUT_DIR, 'taylorswift_cqtList.txt'), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_shift_CQT(M, shiftBins):\n",
    "    \"\"\"\n",
    "        pitchshift equivalent of Prof Tsai's code in matlab\n",
    "        M: a 2-D CQT matrix\n",
    "        shiftBins: An integer that indicates the pitch to shift to\n",
    "        return: a pitchshifted matrix\n",
    "    \"\"\"\n",
    "    shifted = np.roll(M, shiftBins, axis=0)\n",
    "    if shiftBins > 0:\n",
    "        shifted[:shiftBins, :] = 0.\n",
    "    else:\n",
    "        shifted[shiftBins:, :] = 0.\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in f:\n",
    "    logger.debug('==> Generating database for %s' % os.path.basename(line)[:-1])\n",
    "    full_path = os.path.join(DATA_OUTPUT_DIR, line[:-1])\n",
    "    Q = np.load(full_path).T\n",
    "    \n",
    "    # Reshape\n",
    "    width, height = Q.shape\n",
    "    Q = Q[:(width - width%20)].reshape(-1, 1, 20, 121)\n",
    "    \n",
    "    pitch_shift_Qs = np.empty((9, ) + Q.shape)\n",
    "    pitch_shift_Qs[0, :, :] = Q\n",
    "    for i in range(1, 5):\n",
    "        pitch_shift_Qs[i, :, :] = pitch_shift_CQT(Q.T, i).T\n",
    "    for i in range(1, 5):\n",
    "        pitch_shift_Qs[i + 4, :, :] = pitch_shift_CQT(Q.T, -i).T\n",
    "    \n",
    "    fpseqs = np.array([base_model.predict(pitch_shift_Qs[i]) for i in range(pitch_shift_Qs.shape[0])])\n",
    "\n",
    "    key = os.path.basename(line)[:-1]\n",
    "    fpseqs = fpseqs.reshape(9,-1,64)\n",
    "    db.create_dataset(key, fpseqs.shape, np.bool)\n",
    "    db[key][...] = np.where(fpseqs > 0, True, False)\n",
    "f.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = os.path.join(DATA_OUTPUT_DIR, 'taylorswift_db.hdf5')\n",
    "db = h5py.File(db_path, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_paths = get_allpaths('taylorswift', os.path.join(AUDIO_DIR, 'Lists/'), file_type='query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_shape():\n",
    "    '''\n",
    "        returns the shape of query file in (width, height)\n",
    "    '''\n",
    "    assert len(query_paths) > 0\n",
    "    cur_file = query_paths[0]\n",
    "    y, sr = librosa.load(AUDIO_DIR + cur_file + '.wav')\n",
    "    Q = librosa.cqt(y, sr=sr, fmin=130.81, n_bins=121, bins_per_octave=24, hop_length=96)\n",
    "    logQ = preprocess(Q, 3)\n",
    "    return logQ.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_shape = get_query_shape()\n",
    "queries = np.empty((len(query_paths), ) + query_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(query_paths)):\n",
    "    cur_file = query_paths[i]\n",
    "    print('==> Computing CQT of %s'%cur_file)\n",
    "    y, sr = librosa.load(AUDIO_DIR + cur_file + '.wav')\n",
    "    Q = librosa.cqt(y, sr=sr, fmin=130.81, n_bins=121, bins_per_octave=24, hop_length=96)\n",
    "    logQ = preprocess(Q, 3)\n",
    "    queries[i, :, :] = logQ.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = []\n",
    "for query in queries:\n",
    "    width, height = query.shape\n",
    "    query = query[:(width - width%20), :].reshape(-1, 1, 20, 121)\n",
    "    output = np.where(representation_model.predict(query) > 0, True, False)\n",
    "    q.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_querytoref(artist, listdir):\n",
    "    '''\n",
    "        artist: a string representing the artist\n",
    "        listdir: a string representing the directory of the .list file\n",
    "        returns an array of integer, where each entry corresponds to the\n",
    "            reference index\n",
    "    '''\n",
    "\n",
    "    ref_idxs = []\n",
    "    f = open(listdir+artist+'_querytoref.list', 'r')\n",
    "    for line in f:\n",
    "        ref_idx = (line.split(' '))[1]\n",
    "        ref_idxs.append(int(ref_idx))\n",
    "    f.close()\n",
    "    return ref_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = list(db.values())\n",
    "ground_truths = get_querytoref('taylorswift', os.path.join(AUDIO_DIR, 'Lists/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr = calculateMRR(q, refs, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/c8e42144ba622b91fe871332f534b5fa"
  },
  "gist": {
   "data": {
    "description": "Workspace/live-song-id/HashNet-Batchnorm-fixed-dense.ipynb",
    "public": false
   },
   "id": "c8e42144ba622b91fe871332f534b5fa"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
