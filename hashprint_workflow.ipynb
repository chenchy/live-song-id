{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Known-Artist Live Song ID: A Hashprint Approach\n",
    "\n",
    "This notebook is a python implementation of the [original matlab version] (http://pages.hmc.edu/ttsai/assets/livesongid.tar.gz). The pipeline consists of the following modules:\n",
    "\n",
    "1. Preprocessing with Constant-Q Transform (CQT)\n",
    "2. Learning PCA filters\n",
    "3. Obtaining hashprint representation by applying PCA filters to preprocessed CQT matrix\n",
    "4. Generating database with the hashprint of pitch-shifted CQT matrix of reference tracks\n",
    "5. Matching query against database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:01.308607Z",
     "start_time": "2018-02-09T05:03:01.300753Z"
    }
   },
   "outputs": [],
   "source": [
    "GIT_DIR = '/home/zhwang/workspace/live-song-id/'\n",
    "%cd $GIT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "For a given artist, we first preprocess the list of reference tracks by taking CQT transform for each audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:01.983736Z",
     "start_time": "2018-02-09T05:03:01.311809Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "import os\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:01.995822Z",
     "start_time": "2018-02-09T05:03:01.986693Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# audio_path - where audio files are stored\n",
    "# list_dir - where list of softlinks are stored\n",
    "# cqt_dir - where results will be stored\n",
    "audio_path = '/home/zhwang/SoftLinks/' # change this\n",
    "list_dir = audio_path + '/Lists/'\n",
    "cqt_dir = '/home/zhwang/ttemp/livesong_results/' # change this\n",
    "artist = 'taylorswift'\n",
    "out_dir = os.path.join(cqt_dir, artist+'_out')\n",
    "file_paths = get_allpaths(artist, list_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T07:57:29.917108Z",
     "start_time": "2018-02-07T07:41:31.282369Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "%cd $cqt_dir\n",
    "%mkdir $out_dir\n",
    "%cd $out_dir\n",
    "\n",
    "f = open(os.path.join(out_dir, artist + '_cqtList.txt'), 'w')\n",
    "for cur_file in file_paths:\n",
    "    print('==> Computing CQT of %s'%cur_file)\n",
    "    y, sr = librosa.load(audio_path + cur_file + '.wav')\n",
    "    Q = librosa.cqt(y, sr=sr, fmin=130.81, n_bins=121, bins_per_octave=24)\n",
    "    logQ = preprocess(Q, 3)\n",
    "    cur_file_path = os.path.join(os.getcwd(), cur_file + '.dat')\n",
    "    np.savetxt(cur_file_path, logQ)\n",
    "    f.write(cur_file_path + '\\n')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning PCA Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:02.003901Z",
     "start_time": "2018-02-09T05:03:01.998347Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd $GIT_DIR\n",
    "from PCA import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate the filters for the hashprint representation by taking PCA on the covariance matrix for all reference songs for this artist. First, we compute the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:02.010269Z",
     "start_time": "2018-02-09T05:03:02.006385Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 64\n",
    "nbins = 121\n",
    "m = 20 # number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T08:11:10.811072Z",
     "start_time": "2018-02-07T08:09:44.286845Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "f = open(os.path.join(out_dir, artist + '_cqtList.txt'), 'r')\n",
    "accum_cov = np.zeros((nbins * m, nbins * m))\n",
    "count = 0\n",
    "\n",
    "for line in f:\n",
    "    print('==> Computing covariance matrix for %s'%os.path.basename(line)[:-1])\n",
    "    Q = np.loadtxt(line[:-1])\n",
    "    A = getTDE(Q)\n",
    "    if A.shape[1] > 1:\n",
    "        accum_cov += np.cov(A.T)\n",
    "    count += 1\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute PCA from these covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T08:15:04.142290Z",
     "start_time": "2018-02-07T08:14:59.051592Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "evals, evecs = LA.eig(accum_cov / count)\n",
    "ind = np.argsort(-evals)\n",
    "evecs = (evecs[:, ind])[:, :num_features] # this turns out to be complex\n",
    "evecs = np.absolute(evecs)\n",
    "np.savetxt(os.path.join(out_dir, artist + '_model.dat'), evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Filters to CQT Matrix to generate the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:02.300356Z",
     "start_time": "2018-02-09T05:03:02.012706Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evecs = np.loadtxt(os.path.join(out_dir, artist + '_model.dat')).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to stay consistency with the keras model, we transpose the filters and the CQT images to have shape ```(width, height)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:07.264541Z",
     "start_time": "2018-02-09T05:03:02.303705Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_matrix = np.array([vec.reshape((m, -1)) for vec in evecs])\n",
    "delta = 16\n",
    "max_pitch_shift = 4\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each reference track, we pitch shift the CQT matrix by four pitches, both up and down. For each pitch-shifted version, we compute the hashprint by first passing it through the convolutional network, and then taking the delta feature and thresholding by zero.\n",
    "\n",
    "The database contains filenames as keys and the pitch-shifted delta features as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T09:10:01.359591Z",
     "start_time": "2018-02-07T09:09:21.358407Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "f = open(os.path.join(out_dir, artist + '_cqtList.txt'), 'r')\n",
    "db = {}\n",
    "\n",
    "for line in f:\n",
    "    print('==> Generating database for %s'%os.path.basename(line)[:-1])\n",
    "    Q = np.loadtxt(line[:-1]).T\n",
    "    pitch_shift_Qs = np.empty((2 * max_pitch_shift + 1, ) + Q.shape)\n",
    "    pitch_shift_Qs[0, :, :] = Q\n",
    "    for i in range(1, max_pitch_shift + 1):\n",
    "        pitch_shift_Qs[i, :, :] = pitch_shift_CQT(Q.T, i).T\n",
    "    for i in range(1, max_pitch_shift + 1):\n",
    "        pitch_shift_Qs[i + max_pitch_shift, :, :] = pitch_shift_CQT(Q.T, -i).T\n",
    "    \n",
    "    conv_1d_net = build_model(pca_matrix, Q.shape, delta=delta)\n",
    "    \n",
    "    fpseqs = run_model(conv_1d_net, pitch_shift_Qs)\n",
    "    delta_fp = fpseqs[:, :, :fpseqs.shape[2] - delta] - fpseqs[:, :, delta:]\n",
    "    \n",
    "    db[os.path.basename(line)[:-1]] = np.where(delta_fp > 0, 1, 0)\n",
    "f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the database, we serialize it and save it to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:07.270630Z",
     "start_time": "2018-02-09T05:03:07.266797Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "db_path = os.path.join(out_dir, artist + '_db.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-07T09:12:18.842966Z",
     "start_time": "2018-02-07T09:12:17.211091Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with open(db_path, 'wb') as handle:\n",
    "    pickle.dump(db, handle, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Query Against Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we load the database from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:08.118287Z",
     "start_time": "2018-02-09T05:03:07.273712Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from search import *\n",
    "db = {}\n",
    "with open(db_path, 'rb') as handle:\n",
    "    db = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the query by applying the PCA filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:21.647627Z",
     "start_time": "2018-02-09T05:03:08.120388Z"
    }
   },
   "outputs": [],
   "source": [
    "query_paths = get_allpaths(artist, list_dir, file_type='query')\n",
    "def get_query_shape():\n",
    "    '''\n",
    "        returns the shape of query file in (width, height)\n",
    "    '''\n",
    "    assert len(query_paths) > 0\n",
    "    cur_file = query_paths[0]\n",
    "    y, sr = librosa.load(audio_path + cur_file + '.wav')\n",
    "    Q = librosa.cqt(y, sr=sr, fmin=130.81, n_bins=121, bins_per_octave=24)\n",
    "    logQ = preprocess(Q, 3)\n",
    "    return logQ.T.shape\n",
    "\n",
    "query_shape = get_query_shape()\n",
    "querys = np.empty((len(query_paths), ) + query_shape)\n",
    "\n",
    "for i in range(len(query_paths)):\n",
    "    cur_file = query_paths[i]\n",
    "    print('==> Computing CQT of %s'%cur_file)\n",
    "    y, sr = librosa.load(audio_path + cur_file + '.wav')\n",
    "    Q = librosa.cqt(y, sr=sr, fmin=130.81, n_bins=121, bins_per_octave=24)\n",
    "    logQ = preprocess(Q, 3)\n",
    "    querys[i, :, :] = logQ.T\n",
    "\n",
    "conv_1d_net = build_model(pca_matrix, query_shape, delta=delta)\n",
    "fpseqs = run_model(conv_1d_net, querys)\n",
    "delta_fp = fpseqs[:, :, :fpseqs.shape[2] - delta] - fpseqs[:, :, delta:]\n",
    "query_reps = np.where(delta_fp > 0, 1, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run the cross-correlation search algorithm on each of the query representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T05:03:43.086407Z",
     "start_time": "2018-02-09T05:03:21.650998Z"
    }
   },
   "outputs": [],
   "source": [
    "querys = [q for q in query_reps]\n",
    "refs = list(db.values())\n",
    "ground_truths = get_querytoref(artist, list_dir)\n",
    "mrr = calculateMRR(querys, refs, ground_truths)\n",
    "print('==> MRR for %s is %d'%(artist, mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
